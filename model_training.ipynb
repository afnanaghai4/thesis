{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369b307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data related imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
    "\n",
    "from huggingface_hub import login,HfApi, upload_file\n",
    "\n",
    "\n",
    "# classification model imports\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# deep learning model imports\n",
    "import torch\n",
    "import torch.nn as lstm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f849c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:/masters material/thesis/datasets/Edge-IIoTset dataset/Selected dataset for ML and DL/ML-EdgeIIoT-dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0604a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Attack_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef464e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba04233",
   "metadata": {},
   "source": [
    "# decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 1: Drop object columns\n",
    "X = df.drop(columns=['Attack_label'])  # Drop target and any duplicates\n",
    "X = X.select_dtypes(include=['int64', 'float64', 'bool'])  # Keep numeric features only\n",
    "\n",
    "# Step 2: Set target\n",
    "y = df['Attack_label']\n",
    "\n",
    "# Train-val-test split: 70/20/10\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, stratify=y_temp, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fa3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train class distribution:\\n\", y_train.value_counts())\n",
    "print(\"Validation class distribution:\\n\", y_val.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a394c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"X columns:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267241ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,       # Disable the label encoder warning\n",
    "    objective='binary:logistic',   # Important: binary classification objective\n",
    "    eval_metric='logloss'          # Evaluation metric\n",
    ")\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True  # Optional: shows training log\n",
    ")\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "model.save_model(\"xgb_model.json\")\n",
    "\n",
    "# upload_file(\n",
    "#     path_or_fileobj=\"xgb_model.json\",  # or \"xgb_model.pkl\"\n",
    "#     path_in_repo=\"xgb_model.json\",     # File name in the repo\n",
    "#     repo_id=\"ScHemer34/DT_XGBoost\",\n",
    "#     repo_type=\"model\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=['normal', 'attack']))\n",
    "\n",
    "# Print duration\n",
    "training_duration = end_time - start_time\n",
    "print(f\"\\n✅ Model trained in {training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8533c351",
   "metadata": {},
   "source": [
    "# Dataset for Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b16554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afnan\\AppData\\Local\\Temp\\ipykernel_18880\\2324740219.py:1: DtypeWarning: Columns (2,3,6,11,13,14,15,16,17,31,32,34,39,45,51,54,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_fnn = pd.read_csv(\"E:/masters material/thesis/datasets/Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv\")  # adjust path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2219201, 63)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2219201 entries, 0 to 2219200\n",
      "Data columns (total 63 columns):\n",
      " #   Column                     Dtype  \n",
      "---  ------                     -----  \n",
      " 0   frame.time                 object \n",
      " 1   ip.src_host                object \n",
      " 2   ip.dst_host                object \n",
      " 3   arp.dst.proto_ipv4         object \n",
      " 4   arp.opcode                 float64\n",
      " 5   arp.hw.size                float64\n",
      " 6   arp.src.proto_ipv4         object \n",
      " 7   icmp.checksum              float64\n",
      " 8   icmp.seq_le                float64\n",
      " 9   icmp.transmit_timestamp    float64\n",
      " 10  icmp.unused                float64\n",
      " 11  http.file_data             object \n",
      " 12  http.content_length        float64\n",
      " 13  http.request.uri.query     object \n",
      " 14  http.request.method        object \n",
      " 15  http.referer               object \n",
      " 16  http.request.full_uri      object \n",
      " 17  http.request.version       object \n",
      " 18  http.response              float64\n",
      " 19  http.tls_port              float64\n",
      " 20  tcp.ack                    float64\n",
      " 21  tcp.ack_raw                float64\n",
      " 22  tcp.checksum               float64\n",
      " 23  tcp.connection.fin         float64\n",
      " 24  tcp.connection.rst         float64\n",
      " 25  tcp.connection.syn         float64\n",
      " 26  tcp.connection.synack      float64\n",
      " 27  tcp.dstport                float64\n",
      " 28  tcp.flags                  float64\n",
      " 29  tcp.flags.ack              float64\n",
      " 30  tcp.len                    float64\n",
      " 31  tcp.options                object \n",
      " 32  tcp.payload                object \n",
      " 33  tcp.seq                    float64\n",
      " 34  tcp.srcport                object \n",
      " 35  udp.port                   float64\n",
      " 36  udp.stream                 float64\n",
      " 37  udp.time_delta             float64\n",
      " 38  dns.qry.name               float64\n",
      " 39  dns.qry.name.len           object \n",
      " 40  dns.qry.qu                 float64\n",
      " 41  dns.qry.type               float64\n",
      " 42  dns.retransmission         float64\n",
      " 43  dns.retransmit_request     float64\n",
      " 44  dns.retransmit_request_in  float64\n",
      " 45  mqtt.conack.flags          object \n",
      " 46  mqtt.conflag.cleansess     float64\n",
      " 47  mqtt.conflags              float64\n",
      " 48  mqtt.hdrflags              float64\n",
      " 49  mqtt.len                   float64\n",
      " 50  mqtt.msg_decoded_as        float64\n",
      " 51  mqtt.msg                   object \n",
      " 52  mqtt.msgtype               float64\n",
      " 53  mqtt.proto_len             float64\n",
      " 54  mqtt.protoname             object \n",
      " 55  mqtt.topic                 object \n",
      " 56  mqtt.topic_len             float64\n",
      " 57  mqtt.ver                   float64\n",
      " 58  mbtcp.len                  float64\n",
      " 59  mbtcp.trans_id             float64\n",
      " 60  mbtcp.unit_id              float64\n",
      " 61  Attack_label               int64  \n",
      " 62  Attack_type                object \n",
      "dtypes: float64(42), int64(1), object(20)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "df_fnn = pd.read_csv(\"E:/masters material/thesis/datasets/Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv\")  # adjust path\n",
    "print(df_fnn.shape)\n",
    "df_fnn.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm = pd.read_csv(\"E:/masters material/thesis/datasets/Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv\")  # adjust path\n",
    "print(df_lstm.shape)\n",
    "df_lstm.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9d6ea",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.output = nn.Linear(8, 1)  # Output = 1 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output(x)  # No sigmoid here if using BCEWithLogitsLoss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d07d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_fnn.dropna(inplace=True)\n",
    "\n",
    "#encoding important columns\n",
    "# Initialize encoder\n",
    "method_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "df_fnn['http.request.method_encoded'] = method_encoder.fit_transform(df_fnn['http.request.method'].astype(str))\n",
    "df_fnn['http.request.version_encoded'] = np.where(df_fnn['http.request.version'].astype(str).str.strip() == '0', 0, 1)\n",
    "df_fnn['mqtt_topic'] = method_encoder.fit_transform(df_fnn['mqtt.topic'].astype(str))\n",
    "df_fnn['mqtt_protoname'] = method_encoder.fit_transform(df_fnn['mqtt.protoname'].astype(str))\n",
    "df_fnn['Attack_type'] = np.where(df_fnn['Attack_type'].astype(str).str.strip() == 'normal', 0, 1)\n",
    "\n",
    "\n",
    "# Step 2: Set target\n",
    "y_fnn = df_fnn['Attack_label']\n",
    "\n",
    "# Now drop object and unnecessary columns\n",
    "X_fnn = df_fnn.drop(columns=[\n",
    "    'Attack_label', 'http.request.full_uri', 'http.referer', 'http.file_data', \n",
    "    'tcp.payload', 'frame.time', 'mqtt.msg', 'tcp.options', 'dns.qry.name', \n",
    "    'http.request.method', 'http.request.version', 'mqtt.topic', 'mqtt.protoname','ip.src_host',\n",
    "    'ip.dst_host','arp.dst.proto_ipv4','arp.src.proto_ipv4','http.request.uri.query','tcp.srcport',\n",
    "    'dns.qry.name.len','mqtt.conack.flags'\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train-val-test split: 70/20/10\n",
    "X_train_fnn, X_temp_fnn, y_train_fnn, y_temp_fnn = train_test_split(X_fnn, y_fnn, test_size=0.3, stratify=y_fnn, random_state=42)\n",
    "X_val_fnn, X_test_fnn, y_val_fnn, y_test_fnn = train_test_split(X_temp_fnn, y_temp_fnn, test_size=1/3, stratify=y_temp_fnn, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaba97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train class distribution:\\n\", y_train_fnn.value_counts())\n",
    "print(\"Validation class distribution:\\n\", y_val_fnn.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_test_fnn.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_fnn.columns:\n",
    "    if X_fnn[col].apply(type).nunique() > 1:\n",
    "        print(f\"{col}: {X_fnn[col].apply(type).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4614bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fnn.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_fnn = scaler.fit_transform(X_train_fnn)\n",
    "# X_val_fnn = scaler.transform(X_val_fnn)\n",
    "# X_test_fnn = scaler.transform(X_test_fnn)\n",
    "\n",
    "print(X_fnn.dtypes[X_fnn.dtypes == 'object'])\n",
    "\n",
    "# convert dataframe to pytorch tensors\n",
    "X_train_tensor_fnn = torch.tensor(X_train_fnn.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor_fnn = torch.tensor(y_train_fnn.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "X_val_tensor_fnn = torch.tensor(X_val_fnn.to_numpy(), dtype=torch.float32)\n",
    "y_val_tensor_fnn = torch.tensor(y_val_fnn.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time_fnn = time.time()\n",
    "\n",
    "model_fnn = LinearNN(input_size=X_train_fnn.shape[1])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_fnn.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model_fnn.train()\n",
    "    \n",
    "    outputs = model_fnn(X_train_tensor_fnn).squeeze()\n",
    "    loss = criterion(outputs, y_train_tensor_fnn.float())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate validation loss\n",
    "    model_fnn.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        val_outputs = model_fnn(X_val_tensor_fnn).squeeze()\n",
    "        val_loss = criterion(val_outputs, y_val_tensor_fnn.float())\n",
    "\n",
    "    # Print epoch information\n",
    "    model_fnn.train()  # Set model back to training mode\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# End timer\n",
    "end_time_fnn = time.time()\n",
    "\n",
    "torch.save(model_fnn.state_dict(), \"linear_nn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fnn.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test_fnn.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_fnn.values, dtype=torch.float32)\n",
    "\n",
    "    outputs = model_fnn(X_test_tensor).squeeze()\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    preds = (probs > 0.5).float()\n",
    "\n",
    "    correct = (preds == y_test_tensor).sum().item()\n",
    "    accuracy = correct / y_test_tensor.shape[0]\n",
    "\n",
    "print(f\"\\n🧪 Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print duration\n",
    "training_duration_fnn = end_time_fnn - start_time_fnn\n",
    "print(f\"⏱️ Model trained in {training_duration_fnn:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10160086",
   "metadata": {},
   "source": [
    "# Long Short Term Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1f0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(lstm.Module):\n",
    "    def __init__(self,input_size,hidden_size=32):\n",
    "        super(LSTMModel,self).__init__()\n",
    "        self.lstm1 = lstm.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.lstm2 = lstm.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = lstm.Linear(hidden_size,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(x)\n",
    "        out = out[:,-1,:] #take last time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e52b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled_lstm = scaler.fit_transform(X_train_fnn)\n",
    "X_val_scaled_lstm = scaler.transform(X_val_fnn)\n",
    "X_test_scaled_lstm = scaler.transform(X_test_fnn)\n",
    "\n",
    "# Step 2: Convert features to tensors (and unsqueeze for LSTM shape: [batch, seq_len, input_size])\n",
    "X_train_tensor_lstm = torch.tensor(X_train_scaled_lstm, dtype=torch.float32).unsqueeze(2)\n",
    "X_val_tensor_lstm = torch.tensor(X_val_scaled_lstm, dtype=torch.float32).unsqueeze(2)\n",
    "X_test_tensor_lstm = torch.tensor(X_test_scaled_lstm, dtype=torch.float32).unsqueeze(2)\n",
    "\n",
    "# Step 3: Convert targets to tensors (and unsqueeze for [batch_size, 1] if needed)\n",
    "y_train_tensor_lstm = torch.tensor(y_train_fnn.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "y_val_tensor_lstm = torch.tensor(y_val_fnn.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor_lstm = torch.tensor(y_test_fnn.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Step 4: Set the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# 3. Wrap tensors into a Dataset\n",
    "train_dataset = TensorDataset(X_train_tensor_lstm, y_train_tensor_lstm)\n",
    "val_dataset = TensorDataset(X_val_tensor_lstm, y_val_tensor_lstm)\n",
    "\n",
    "# 4. Use DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "759b1bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.0044, Val Loss: 0.0000\n",
      "Epoch [2/30], Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m model_lstm\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     20\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:629\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "input_size = X_train_tensor_lstm.shape[2]\n",
    "model_lstm = LSTMModel(input_size=input_size)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to device\n",
    "model_lstm = model_lstm.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "start_time_lstm = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lstm(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model_lstm.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            val_outputs = model_lstm(X_batch)\n",
    "            loss = criterion(val_outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "end_time_lstm = time.time()\n",
    "training_duration_lstm = end_time_lstm - start_time_lstm\n",
    "\n",
    "# Save the model\n",
    "torch.save(model_lstm.state_dict(), \"lstm_model.pth\")\n",
    "print(f\"\\nTraining time: {training_duration_lstm:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model_lstm(X_test_tensor_lstm)\n",
    "    predicted = torch.sigmoid(test_outputs)\n",
    "    predicted_classes = (predicted >= 0.5).float()\n",
    "\n",
    "    accuracy = (predicted_classes == y_test_tensor_lstm).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Training duration: {training_duration_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dc578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fc2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
